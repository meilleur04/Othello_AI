{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from keras.optimizers import Adam,RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "%run AIPlayer.ipynb\n",
    "%run SmallGame.ipynb\n",
    "%run Game.ipynb\n",
    "%run DDQNAgent.ipynb\n",
    "%run SmallAgent.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '8x8_weights_new.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 179\u001b[0m\n\u001b[0;32m    176\u001b[0m     plot_seaborn_predicted(counter_plot,counter_predicted)\n\u001b[0;32m    177\u001b[0m     plot_seaborn_wrong(counter_plot,wrong_plot)\n\u001b[1;32m--> 179\u001b[0m run()\n",
      "Cell \u001b[1;32mIn[57], line 65\u001b[0m, in \u001b[0;36mrun\u001b[1;34m()\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m():\n\u001b[1;32m---> 65\u001b[0m     agent \u001b[39m=\u001b[39m DQNAgent()\n\u001b[0;32m     66\u001b[0m     \u001b[39mprint\u001b[39m(agent\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39msummary())\n\u001b[0;32m     67\u001b[0m     counter_games \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3040\\2262652201.py:19\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39m=\u001b[39m Memory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_cap)\n\u001b[0;32m     18\u001b[0m \u001b[39m#self.model = self.network()\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(\u001b[39m\"\u001b[39;49m\u001b[39m8x8_weights_new.hdf5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     21\u001b[0m \u001b[39m#model for target network\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m#self.model_=self.network()\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork(\u001b[39m\"\u001b[39m\u001b[39m8x8_weights_new.hdf5\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3040\\2262652201.py:38\u001b[0m, in \u001b[0;36mDQNAgent.network\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m     35\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mHuber(), optimizer\u001b[39m=\u001b[39mopt)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m weights:\n\u001b[1;32m---> 38\u001b[0m     model\u001b[39m.\u001b[39;49mload_weights(weights)\n\u001b[0;32m     39\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\h5py\\_hl\\files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    525\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    527\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[0;32m    528\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[0;32m    529\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    530\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    531\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[0;32m    532\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 533\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\h5py\\_hl\\files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[0;32m    225\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 226\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[0;32m    227\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    228\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '8x8_weights_new.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "#Training phase for 8x8 Agent\n",
    "def initialize_game(agent):\n",
    "    print(\"Starting random agent:\")\n",
    "    #creating memory for PER with random matches\n",
    "    i=0\n",
    "    while i < agent.memory_cap:\n",
    "        b=Board(verbose=False)\n",
    "        pl=AI(0)\n",
    "        opp=AI(0)\n",
    "        d=b.get_game_over()\n",
    "        while d==False and i< agent.memory_cap:\n",
    "            state_init1 = agent.get_state(b)\n",
    "            action=pl.move(b)\n",
    "            reward=agent.set_reward(b,action)\n",
    "            b.coord_move(action)\n",
    "            d=b.get_game_over()\n",
    "            if(d==False):\n",
    "                b.coord_move(opp.move(b))\n",
    "                next_state= agent.get_state(b)\n",
    "                d=b.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([64])\n",
    "            next_state= agent.get_state(b)\n",
    "            d=b.get_game_over()\n",
    "            sample=(state_init1, action,reward, next_state, d)\n",
    "            #I use the reward as error to remember in the memory\n",
    "            error=abs(sample[2])\n",
    "            i+=1\n",
    "            agent.memory.add(error,sample)\n",
    "    print(\"Ending of Random creating memory\")\n",
    "    \n",
    "    \n",
    "def plot_seaborn_score(array_counter, array_score):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='score')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_win(array_counter, array_win):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_win])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='wins')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_predicted(array_counter, array_pred):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_pred])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='number of predicted moves')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_wrong(array_counter, array_wrong):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_wrong])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='times where i did a not valid move')\n",
    "    plt.show()\n",
    "    \n",
    "def get_record(score, record):\n",
    "        if score >= record:\n",
    "            return score\n",
    "        else:\n",
    "            return record \n",
    "    \n",
    "def run():\n",
    "\n",
    "    agent = DQNAgent()\n",
    "    print(agent.model.summary())\n",
    "    counter_games = 0\n",
    "    counter_move = 0\n",
    "    counter_notfin = 0\n",
    "    score_plot = []\n",
    "    win_plot = []\n",
    "    wrong_plot = []\n",
    "    counter_plot =[]\n",
    "    counter_predicted=[]\n",
    "    record = 0\n",
    "    #filling the memory\n",
    "    initialize_game(agent)\n",
    "        \n",
    "    while counter_games < 8000:\n",
    "        # Initialize classes\n",
    "        board=Board(verbose=False)\n",
    "        opponent = AI(0)\n",
    "        counter_pred = 0        \n",
    "        wrong_move=False\n",
    "        \n",
    "        done=board.get_game_over()\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            #get old state\n",
    "            state_old = agent.get_state(board)\n",
    "            counter_move +=1\n",
    "            \n",
    "            #predict the move of the agent\n",
    "            if random.random() < agent.epsilon:\n",
    "                if(agent.epsilon < 0.01):\n",
    "                    print(\"Possible move:\")\n",
    "                    state_old = agent.get_state(board)\n",
    "                    possible_moves = board.get_valid_moves()  \n",
    "                    possible_moves=list(zip(*possible_moves))\n",
    "                    weights = np.zeros(len(possible_moves[0]))\n",
    "                    p = np.exp(weights)\n",
    "                    p /= np.sum(p)\n",
    "                    choice = np.random.choice(range(len(weights)), p=p)\n",
    "                    action = possible_moves[0][choice]\n",
    "                else:\n",
    "                    print(\"Random move:\")\n",
    "                    #giving out also wrong possibilities to get also their values for the q-function\n",
    "                    mov=random.randint(0,63)\n",
    "                    row=mov//8\n",
    "                    column=mov%8\n",
    "                    action=(row,column)\n",
    "            else:\n",
    "                print(\"Predicted move:\")\n",
    "                counter_pred +=1\n",
    "                state_old = agent.get_state(board)\n",
    "                pred=agent.predict(state_old.reshape((1,64)))[0]\n",
    "                mov=np.argmax(pred)\n",
    "                row=mov//8            \n",
    "                column=mov%8\n",
    "                action=(row,column)\n",
    "    \n",
    "            #get the reward of the move predicted\n",
    "            reward= agent.set_reward(board,action)\n",
    "                \n",
    "            #perform new move and get new state\n",
    "            #I also get if the game is ended or not\n",
    "            print(action)\n",
    "            try:\n",
    "                board.coord_move(action)\n",
    "                done=board.get_game_over()\n",
    "            except:\n",
    "                print(\"Player did a not valid move\")\n",
    "                #ending manually the loop\n",
    "                wrong_move=True\n",
    "                done=True\n",
    "                \n",
    "            #opponent make move\n",
    "            #the next state is the state after the move of the opponent so it's again the move of the agent\n",
    "            if(done==False):\n",
    "                board.coord_move(opponent.move(board))\n",
    "                next_state= agent.get_state(board)\n",
    "                #inspecting if the game ended after the opponent move\n",
    "                done=board.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([64])\n",
    "            \n",
    "            # store the new data into a long term memory for the all game\n",
    "            agent.remember(state_old, action,reward, next_state, done)\n",
    "            \n",
    "            #train with replay new\n",
    "            agent.replay()\n",
    "            \n",
    "            #save score for final plot\n",
    "            record = get_record(board.get_black_score(), record)           \n",
    "            \n",
    "        counter_games += 1\n",
    "        print('Game', counter_games, '      Score:', board.get_black_score())\n",
    "        score_plot.append(board.get_black_score())\n",
    "        if board.get_black_score()>=board.get_white_score():\n",
    "            if wrong_move==True:\n",
    "                #I did a non valid move and just for a case I had more points but i lost\n",
    "                win_plot.append(0)\n",
    "                wrong_plot.append(1)\n",
    "            else:\n",
    "                win_plot.append(1)\n",
    "                wrong_plot.append(0)\n",
    "        else:\n",
    "            win_plot.append(0)\n",
    "            wrong_plot.append(0)\n",
    "        counter_plot.append(counter_games)\n",
    "        counter_predicted.append(counter_pred)\n",
    "    agent.model.save_weights('8x8_weights_new.hdf5')\n",
    "    plot_seaborn_score(counter_plot, score_plot)\n",
    "    plot_seaborn_win(counter_plot,win_plot)\n",
    "    plot_seaborn_predicted(counter_plot,counter_predicted)\n",
    "    plot_seaborn_wrong(counter_plot,wrong_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training phase for 4x4 Agent\n",
    "def initialize_gameS(agent):\n",
    "    print(\"Starting random agent:\")\n",
    "    #creating memory for PER with random matches\n",
    "    i=0\n",
    "    while i < agent.memory_cap:\n",
    "        b=SmallBoard(verbose=False)\n",
    "        #b=Board(verbose=False)\n",
    "        pl=AI(0)\n",
    "        opp=AI(0)\n",
    "        d=b.get_game_over()\n",
    "        while d==False and i< agent.memory_cap:\n",
    "            state_init1 = agent.get_state(b)\n",
    "            action=pl.move(b)\n",
    "            reward=agent.set_reward(b,action)\n",
    "            b.coord_move(action)\n",
    "            d=b.get_game_over()\n",
    "            if(d==False):\n",
    "                b.coord_move(opp.move(b))\n",
    "                next_state= agent.get_state(b)\n",
    "                d=b.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([16])\n",
    "            next_state= agent.get_state(b)\n",
    "            d=b.get_game_over()\n",
    "            sample=(state_init1, action,reward, next_state, d)\n",
    "            #I use the reward as error to remember in the memory\n",
    "            error=abs(sample[2])\n",
    "            i+=1\n",
    "            agent.memory.add(error,sample)\n",
    "    print(\"Ending of Random creating memory\")\n",
    "    \n",
    "    \n",
    "def plot_seaborn_scoreS(array_counter, array_score):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_score])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='score')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_winS(array_counter, array_win):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_win])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='wins')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_predictedS(array_counter, array_pred):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_pred])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='number of predicted moves')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_seaborn_wrongS(array_counter, array_wrong):\n",
    "    sns.set(color_codes=True)\n",
    "    ax = sns.regplot(np.array([array_counter])[0], np.array([array_wrong])[0], color=\"b\", x_jitter=.1, line_kws={'color':'green'})\n",
    "    ax.set(xlabel='games', ylabel='times where i did a not valid move')\n",
    "    plt.show()\n",
    "    \n",
    "def get_recordS(score, record):\n",
    "        if score >= record:\n",
    "            return score\n",
    "        else:\n",
    "            return record \n",
    "    \n",
    "def runS():\n",
    "\n",
    "    agent = DQNSmallAgent()\n",
    "    print(agent.model.summary())\n",
    "    counter_games = 0\n",
    "    counter_move = 0\n",
    "    counter_notfin = 0\n",
    "    score_plot = []\n",
    "    win_plot = []\n",
    "    wrong_plot = []\n",
    "    counter_plot =[]\n",
    "    counter_predicted=[]\n",
    "    record = 0\n",
    "    #filling the memory\n",
    "    initialize_gameS(agent)\n",
    "        \n",
    "    while counter_games < 7000:\n",
    "        # Initialize classes\n",
    "        board = SmallBoard(verbose=False)\n",
    "        #b=Board(verbose=False)\n",
    "        opponent = AI(0)\n",
    "        counter_pred = 0        \n",
    "        wrong_move=False\n",
    "        \n",
    "        done=board.get_game_over()\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            #get old state\n",
    "            state_old = agent.get_state(board)\n",
    "            counter_move +=1\n",
    "            \n",
    "            #predict the move of the agent\n",
    "            #action= agent.makemove(board)\n",
    "            if random.random() < agent.epsilon:\n",
    "                if(agent.epsilon < 0.01):\n",
    "                    print(\"Possible move:\")\n",
    "                    state_old = agent.get_state(board)\n",
    "                    possible_moves = board.get_valid_moves()  \n",
    "                    possible_moves=list(zip(*possible_moves))\n",
    "                    weights = np.zeros(len(possible_moves[0]))\n",
    "                    p = np.exp(weights)\n",
    "                    p /= np.sum(p)\n",
    "                    choice = np.random.choice(range(len(weights)), p=p)\n",
    "                    action = possible_moves[0][choice]\n",
    "                else:\n",
    "                    print(\"Random move:\")\n",
    "                    #giving out also wrong possibilities to get also their values for the q-function\n",
    "                    mov=random.randint(0,15)\n",
    "                    row=mov//4\n",
    "                    column=mov%4\n",
    "                    action=(row,column)\n",
    "            else:\n",
    "                print(\"Predicted move:\")\n",
    "                counter_pred +=1\n",
    "                state_old = agent.get_state(board)\n",
    "                pred=agent.predict(state_old.reshape((1,16)))[0]\n",
    "                mov=np.argmax(pred)\n",
    "                row=mov//4\n",
    "                column=mov%4\n",
    "                action=(row,column)\n",
    "    \n",
    "            #get the reward of the move predicted\n",
    "            reward= agent.set_reward(board,action)\n",
    "                \n",
    "            #perform new move and get new state\n",
    "            #I also get if the game is ended or not\n",
    "            print(action)\n",
    "            try:\n",
    "                board.coord_move(action)\n",
    "                done=board.get_game_over()\n",
    "            except:\n",
    "                print(\"Player did a not valid move\")\n",
    "                #ending manually the loop\n",
    "                wrong_move=True\n",
    "                done=True\n",
    "                \n",
    "            #opponent make move\n",
    "            #the next state is the state after the move of the opponent so it's again the move of the agent\n",
    "            if(done==False):\n",
    "                board.coord_move(opponent.move(board))\n",
    "                next_state= agent.get_state(board)\n",
    "                #inspecting if the game ended after the opponent move\n",
    "                done=board.get_game_over()\n",
    "            else:\n",
    "                next_state=np.zeros([16])\n",
    "            \n",
    "            # store the new data into a long term memory for the all game\n",
    "            agent.remember(state_old, action,reward, next_state, done)\n",
    "            \n",
    "            #train with replay new\n",
    "            agent.replay()\n",
    "            \n",
    "            #save score for final plot\n",
    "            record = get_recordS(board.get_black_score(), record)           \n",
    "            \n",
    "        counter_games += 1\n",
    "        print('Game', counter_games, '      Score:', board.get_black_score())\n",
    "        score_plot.append(board.get_black_score())\n",
    "        if board.get_black_score()>=board.get_white_score():\n",
    "            if wrong_move==True:\n",
    "                #I did a non valid move and just for a case I had more points but i lost\n",
    "                win_plot.append(0)\n",
    "                wrong_plot.append(1)\n",
    "            else:\n",
    "                win_plot.append(1)\n",
    "                wrong_plot.append(0)\n",
    "        else:\n",
    "            win_plot.append(0)\n",
    "            wrong_plot.append(0)\n",
    "        counter_plot.append(counter_games)\n",
    "        counter_predicted.append(counter_pred)\n",
    "    agent.model.save_weights('4x4_weights_new.hdf5')\n",
    "    plot_seaborn_scoreS(counter_plot, score_plot)\n",
    "    plot_seaborn_winS(counter_plot,win_plot)\n",
    "    plot_seaborn_predictedS(counter_plot,counter_predicted)\n",
    "    plot_seaborn_wrongS(counter_plot,wrong_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGames():\n",
    "    b = SmallBoard()\n",
    "    agent = DQNSmallAgent(\"4x4_weights_new.hdf5\")\n",
    "    opp = AI(0)\n",
    "    counter_games = 0\n",
    "    win_plot = []\n",
    "    counter_plot = []\n",
    "    counter_wrong = 0\n",
    "    counter_pred = 0\n",
    "    \n",
    "    while counter_games < 1000:\n",
    "        wrong_move = False\n",
    "        i = 0\n",
    "        b = SmallBoard()\n",
    "        d = b.get_game_over()\n",
    "        \n",
    "        while not d:\n",
    "            if i == 0:\n",
    "                try:\n",
    "                    move = agent.makemove(b)\n",
    "                    print(move)\n",
    "                    if not b.is_valid_move(move):\n",
    "                        print(\"Invalid move by the agent\")\n",
    "                        wrong_move = True\n",
    "                        counter_wrong += 1\n",
    "                        break\n",
    "                    b.coord_move(move)\n",
    "                    counter_pred += 1\n",
    "                except Exception as e:\n",
    "                    print(\"Error:\", e)\n",
    "                    wrong_move = True\n",
    "                    counter_wrong += 1\n",
    "                    break\n",
    "                i = 1\n",
    "            else:\n",
    "                opp_move = opp.move(b)\n",
    "                if not b.is_valid_move(opp_move):\n",
    "                    print(\"Invalid move by the opponent\")\n",
    "                    wrong_move = True\n",
    "                    counter_wrong += 1\n",
    "                    break\n",
    "                b.coord_move(opp_move)\n",
    "                i = 0\n",
    "            \n",
    "            d = b.get_game_over()\n",
    "        \n",
    "        if not wrong_move:\n",
    "            if b.get_black_score() >= b.get_white_score():\n",
    "                win_plot.append(1)\n",
    "            else:\n",
    "                win_plot.append(0)\n",
    "        \n",
    "        counter_games += 1\n",
    "        counter_plot.append(counter_games)\n",
    "    \n",
    "    plot_seaborn_win(counter_plot, win_plot)\n",
    "    \n",
    "    if counter_pred > 0:\n",
    "        print(\"The percentage of wrong moves is {:.4f}%\".format(counter_wrong / counter_pred * 100))\n",
    "    else:\n",
    "        print(\"No predictions made, so percentage of wrong moves is not applicable.\")\n",
    "    \n",
    "    print(\"The number of wrong moves is {}\".format(counter_wrong))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the Q-function\n",
    "def plotQ():\n",
    "    b=SmallBoard()\n",
    "    agent = DQNSmallAgent(\"4x4_randomweights_3layers_good.hdf5\")\n",
    "    agentBad =DQNSmallAgent(\"4x4_weights_smalltrain.hdf5\")\n",
    "    b.human_move(\"B1\")\n",
    "    b.human_move(\"A3\")\n",
    "    state_old = agent.get_state(b)\n",
    "    pred=agent.model.predict(state_old.reshape((1,16)))[0]\n",
    "    predBad=agentBad.model.predict(state_old.reshape((1,16)))[0]\n",
    "    mov=np.argmax(pred)\n",
    "    row=mov//4\n",
    "    column=mov%4\n",
    "    action=(row,column)\n",
    "    b.coord_move(action)\n",
    "    print(\"\\nQ-function of trained agent:\")\n",
    "    print(pred[0],\"\\t\",pred[1],\"\\t\" ,pred[2],\"\\t\",pred[3])\n",
    "    print(pred[4],\"\\t\", pred[5],\"\\t\", pred[6],\"\\t\", pred[7])\n",
    "    print(pred[8],\"\\t\", pred[9],\"\\t\", pred[10],\"\\t\", pred[11])\n",
    "    print(pred[12],\"\\t\", pred[13],\"\\t\", pred[14],\"\\t\", pred[15])\n",
    "    print(\"\\nQ-function of short trained agent:\")\n",
    "    print(predBad[0],\"\\t\",predBad[1],\"\\t\" ,predBad[2],\"\\t\",predBad[3])\n",
    "    print(predBad[4],\"\\t\", predBad[5],\"\\t\", predBad[6],\"\\t\", predBad[7])\n",
    "    print(predBad[8],\"\\t\", predBad[9],\"\\t\", predBad[10],\"\\t\", predBad[11])\n",
    "    print(predBad[12],\"\\t\", predBad[13],\"\\t\", predBad[14],\"\\t\", predBad[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ◦ ◦ ◦ ◦\n",
      "2 ◦ ○ ● ◦\n",
      "3 ◦ ● ○ ◦\n",
      "4 ◦ ◦ ◦ ◦\n",
      "  A B C D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to open file (unable to open file: name = '4x4_weights_new.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m runGames()\n",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m, in \u001b[0;36mrunGames\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrunGames\u001b[39m():\n\u001b[0;32m      2\u001b[0m     b \u001b[39m=\u001b[39m SmallBoard()\n\u001b[1;32m----> 3\u001b[0m     agent \u001b[39m=\u001b[39m DQNSmallAgent(\u001b[39m\"\u001b[39;49m\u001b[39m4x4_weights_new.hdf5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m     opp \u001b[39m=\u001b[39m AI(\u001b[39m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m     counter_games \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3040\\2608036058.py:20\u001b[0m, in \u001b[0;36mDQNSmallAgent.__init__\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_target_freq\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39m=\u001b[39m Memory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_cap)\n\u001b[1;32m---> 20\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnetwork(weights)\n\u001b[0;32m     21\u001b[0m \u001b[39m#self.model = self.network(\"4x4_randomweights_3layers_good.hdf5\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[39m#model for target network\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork(weights)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3040\\2608036058.py:42\u001b[0m, in \u001b[0;36mDQNSmallAgent.network\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m     39\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mHuber(), optimizer\u001b[39m=\u001b[39mopt)\n\u001b[0;32m     41\u001b[0m \u001b[39mif\u001b[39;00m weights:\n\u001b[1;32m---> 42\u001b[0m     model\u001b[39m.\u001b[39;49mload_weights(weights)\n\u001b[0;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\h5py\\_hl\\files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[0;32m    525\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    527\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[0;32m    528\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[0;32m    529\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    530\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    531\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[0;32m    532\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 533\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39;49mswmr)\n\u001b[0;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32mc:\\Users\\gurma\\anaconda3\\envs\\tensorflowenv\\lib\\site-packages\\h5py\\_hl\\files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[0;32m    225\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 226\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[0;32m    227\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    228\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to open file (unable to open file: name = '4x4_weights_new.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "runGames()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
